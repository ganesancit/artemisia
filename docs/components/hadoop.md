
 
Hadoop
======

This component provides support for basic hadoop related tasks

| Task         | Description                                                        |
|--------------|--------------------------------------------------------------------|
| HDFSTask     | This task lets you execute DFS shell commands supported by HDFS    |
| SqoopExport  | Launch Sqoop job to export data from database to HDFS filesystem.  |

     

 
### HDFSTask:


#### Description:

 
HDFSTask lets you perform DFS related operations. This tasks composes the HDFS shell commands
and executes it. Some of the commands that you can execute are like
  * chmod
  * copyFromFile
  * rmdir
    

#### Configuration Structure:


      {
        Component = "Hadoop"
        Task = "HDFSTask"
        params =  {
         action = "copyFromLocal @required"
         args = ["/user/artemisia/srcfile", "/var/path/target_file"]
         hdfs-bin = "/usr/bin/hdfs @optional"
      }
     }


#### Field Description:

 * hdfs-bin: path to the hdfs/hadoop binary file
 * action: dfs action to be performed. supported actions are 
     * chmod
     * chown
     * copyFromLocal
 * args: arguments to be passed

#### Task Output:


     {
        stderr = "stderr of the hdfs command"
        stdout = "stdout of the hdfs command"
     }

 
The resultant HDFS command's stdout and stderr is included in the task's output.
    
           

     




### SqoopExport:


#### Description:

 
 This task helps you to export data from any RDBMS systems to HDFS using Sqoop. This task expects the Sqoop
 application to be installed locally your machine. This task composes the sqoop command to launch using the
 params config. You can optionally specify  **sqoop-bin** field to provide the path to the sqoop binary file.
 make sure this is plain *sqoop* file and not *sqoop-import* file. This Sqoop binary file can be inferred from the
 PATH system variable if found. This field is required only if the Sqoop file is missing in the PATH variable.
 similarly to **sqoop-bin**, **hdfs-bin** field can be used to refer to the hdfs binary file. **hdfs-bin** field
 is optional as well and required only if the *hdfs* file is not found in the PATH variable. The path is searched
 for two files hdfs or hadoop and uses either of them.

 The Sqoop export supports quoting via **export.quoting** field. but this field is applicable only when the
 format mode is set to *text*. The field **misc-setting** lets you set any miscellaneous arguments you want the
 Sqoop job to take, but was not possible to set with other provided arguments. for eg if you had to set both
 *direct-split-size* and *append* arguments to the Sqoop job you there is no direct way to set it since such fields
 are not supported by this task. One way this could be achieved is by using the **misc-setting**. The **misc-setting**
 field takes a list of values. Each item in the list can either be a string or an object with a single key value.
 so as said earlier if we wanted to add *direct-split-size* and *append* argument to the sqoop job our misc-options
 field would look like.

        misc-setting = [ { direct-split-size = 1000 }, append ]

 The above config would translate to `--direct-split-size 1000 --append` arguments being added to the command line.
 the misc setting can be used to override setting generated by other fields. for eg **sqoop-setting.num-mappers** field
 would translate to --num-mappers command. so now with the below config the effective num-mappers will be 10 and not 5.

             sqoop-setting = {
               num-mappers = 5
             }
             misc-setting = [
               { num-mappers = 10 },
               -Djob.mapred.name=my_custom_job_name
             ]

 Please note that you can only override key value arguments like *--num-mappers 10* with new value. you cannot remove
 any arguments that was generated by other settings like *sqoop-setting*, *connection*, *export* etc using *misc-setting*.
 Make sure minus D arguments (-D) are represented as a single argument as shown above without any space between D and
 the argument name. If any of the arguments had leading dash(es) already defined then the arguments are taken as is.
 For eg in the above config object the map-reduce name argument has a leading dash so no more dashses are added to the
 above argument whereas the num-mappers will have a double dash prepended as **--num-mappers**
    

#### Configuration Structure:


      {
        Component = "Hadoop"
        Task = "SqoopExport"
        params =  {
         connection =   {
           type = ""
        }
         connection.dsn_[1] = "connection-name"
         connection.dsn_[2] =   {
           database = "db @required"
           host = "db-host @required"
           password = "password @required"
           port = "100 @required"
           username = "username @required"
        }
         export =   {
           delimiter = "| @default(,) @type(char)"
           escapechar = "'\\' @default(\\) @type(char)"
           format = "text"
           quotechar = "'\"' @default(\") @type(char)"
           quoting = "yes @default(false) @type(boolean)"
        }
         hdfs-bin = "/var/path/bin/hdfs @optional"
         misc-setting = ["-Dmapreduce.map.speculative=true",    {
            direct-split-size = "10000"
         }]
         sqoop-bin = "/var/path/bin/sqoop @optional"
         sqoop-setting =   {
           queue-name = "queue-name @default(default)"
           split-by = "id @optional"
           sql = "SELECT * FROM table @optional"
           src-table = "database.tablename @optional"
           target = "/user/artemisia/file.txt"
           target-type = "hive @default(hdfs)"
           truncate = "yes @default(no)"
           where-clause = "id > 100 @optional"
        }
      }
     }


#### Field Description:

 * sqoop-setting:
    * sql: source sql to be exported. either this field or the *src-table* field must be provided
    * where-clause: where clause to be applied on *src-table*. applicable only when *src-table* is set
    * src-table: source table to be exported. either this field or the *sql* field must be provided
    * truncate: truncate the target hdfs directory or target hive table
    * split-by: column to be used split
    * target-type: the target type. the supported target types are
        * hdfs
        * hive
    * target: this is either hdfs path or hive tablename depending on target-type set
    * queue-name: name of the queue where the MR job will be submitted
 * sqoop-bin: optional path to the Sqoop binary. This field is optional and not required if the sqoop binary is already available in the PATH variable
 * export:
    * format: output data format. allowed values are text,sequence,avro
    * quotechar: quotechar to use if quoting is enabled.
    * sqlfile: used in place of sql key to pass the file containing the SQL
    * escapechar: escape character use for instance to escape delimiter values in field
    * quoting: boolean literal to enable/disable quoting of fields.
    * delimiter: character to be used for delimiter
 * connection: connection info for the source database is provided here. it has two fields
     * type: type of the database source. for eg: mysql, postgres etc.
     * dsn: connection information to connect to the database. DSN name or inline connection object
 * hdfs-bin: hdfs binary to use. This field is optional and not required if the hdfs or hadoop binary is already available in the PATH variable.
 * misc-setting: any miscellenous options to be applied to the sqoop command.

#### Task Output:


     {
        taskname =  {
         __stats__ =   {
           input-rows = 100
           output-rows = 100
        }
      }
     }

 
**taskname.__stats__.input-rows** is the total no of rows read from the database (Map Output rows) and
**taskname.__stats__.output-rows** is the total no of rows written to the output file (Map Input rows).
Here we assume the taskname is the name of your task.
    
           

     

     